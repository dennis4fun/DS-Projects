name: Stock Price Forecasting CI/CD

# Controls when the workflow will run
on:
  # Triggers the workflow on push events to the 'features/CI-DS' branch for testing.
  # Once stable, change 'features/CI-DS' to 'main' or add 'main' to the list.
  push:
    branches: [features/CI-DS] # <--- CURRENTLY SET TO YOUR FEATURE BRANCH

  # Allows you to run this workflow manually from the Actions tab in GitHub
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # Job 1: Data Scraping
  data_scraping:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4 # Action to check out your repository code
        # Repository root is the default working directory

      - name: Set up Python environment
        uses: actions/setup-python@v5 # Action to set up Python
        with:
          python-version: '3.12' # Your confirmed Python version (3.12.1 is compatible with 3.12)

      - name: Install pipenv
        run: pip install pipenv # Install pipenv globally in the runner

      - name: Install project dependencies
        # Run pipenv install from the repository root where Pipfile/Pipfile.lock reside
        run: pipenv install --deploy --system
        # No 'working-directory' needed here as Pipfile is at repo root

      # No 'mkdir -p data' needed here, as your Python script handles directory creation at the root.
      - name: Run Data Scraping Script
        # Execute pipenv run python from the repository root, providing the full path to the script.
        run: pipenv run python CI-DS-Forecasting/get_stock_data.py
        # No 'working-directory' needed, as we're running from the root.

      - name: Upload Raw Stock Data as Artifact
        uses: actions/upload-artifact@v4 # Action to upload artifacts
        with:
          name: raw-stock-data # Name of the artifact
          path: data/raw_stock_data.csv # Path from repo root (as your script creates it here)
          retention-days: 1

  # Job 2: Data Processing and EDA
  data_processing_eda:
    runs-on: ubuntu-latest
    needs: data_scraping
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Raw Stock Data Artifact
        uses: actions/download-artifact@v4
        with:
          name: raw-stock-data
          path: data/ # Downloads to this path from repo root

      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install pipenv
        run: pip install pipenv

      - name: Install project dependencies
        run: pipenv install --deploy --system

      # No 'mkdir -p data/plots' needed, as your Python script handles directory creation.

      - name: Run Data Cleaning and EDA Script
        run: pipenv run python CI-DS-Forecasting/data_cleaning_EDA.py

      - name: Upload Processed Data and EDA Plots Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: processed-data-and-eda-plots
          path: |
            data/processed_data.csv # Path from repo root
            data/plots/ # Path from repo root
          retention-days: 1

  # Job 3: Model Training
  model_training:
    runs-on: ubuntu-latest
    needs: data_processing_eda
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Processed Data Artifact
        uses: actions/download-artifact@v4
        with:
          name: processed-data-and-eda-plots
          path: data/ # Downloads to this path

      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install pipenv
        run: pip install pipenv

      - name: Install project dependencies
        run: pipenv install --deploy --system

      # No 'mkdir -p ml_artifacts' needed, as your Python script handles directory creation.

      - name: Run Model Training Script
        run: pipenv run python CI-DS-Forecasting/model_training.py

      - name: Upload Trained Models Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: trained-models
          path: ml_artifacts/prophet_model_*.joblib # Path from repo root
          retention-days: 1

  # Job 4: Forecasting and Reporting
  forecasting_reporting:
    runs-on: ubuntu-latest
    needs: model_training
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Processed Data Artifact
        uses: actions/download-artifact@v4
        with:
          name: processed-data-and-eda-plots
          path: data/ # Downloads to this path

      - name: Download Trained Models Artifacts
        uses: actions/download-artifact@v4
        with:
          name: trained-models
          path: ml_artifacts/ # Downloads to this path

      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install pipenv
        run: pip install pipenv

      - name: Install project dependencies
        run: pipenv install --deploy --system

      - name: Run Forecasting and Reporting Script
        run: pipenv run python CI-DS-Forecasting/ml_forecasting.py

      - name: Upload Final Report Plots
        uses: actions/upload-artifact@v4
        with:
          name: final-plots-report # Name for the final artifact group
          path: ml_artifacts/*.png # Path from repo root
          retention-days: 7
