name: Stock Price Forecasting CI/CD

# Controls when the workflow will run
on:
  # Triggers the workflow on push events to the 'main' branch.
  # For initial testing, you might want to change 'main' to 'features/CI-DS'
  # Once stable, change it back to 'main' or add 'main' here and delete the feature branch.
  push:
    branches: [feature/CI-DS] # [main] Consider changing to [ features/CI-DS ] for initial development

  # Allows you to run this workflow manually from the Actions tab in GitHub
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # Job 1: Data Scraping
  data_scraping:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4 # Action to check out your repository code

      - name: Set up Python environment
        uses: actions/setup-python@v5 # Action to set up Python
        with:
          python-version: '3.9' # Specify the Python version matching your Pipfile

      - name: Install pipenv
        run: pip install pipenv # Install pipenv globally in the runner

      - name: Install project dependencies
        # Use --deploy for CI/CD to ensure exact lock file dependencies are installed
        # --system flag integrates the virtual environment with the system Python,
        # making it accessible directly by `pipenv run` without path issues in CI.
        run: pipenv install --deploy --system

      - name: Create data directory
        run: mkdir -p data # Ensure the data directory exists before the script runs

      - name: Run Data Scraping Script
        run: pipenv run python get_stock_data.py # Execute your data scraping script

      - name: Upload Raw Stock Data as Artifact
        uses: actions/upload-artifact@v4 # Action to upload artifacts
        with:
          name: raw-stock-data # Name of the artifact
          path: data/raw_stock_data.csv # Path to the file(s) to upload
          retention-days: 1 # How long to keep the artifact (adjust as needed for debugging/storage)

  # Job 2: Data Processing and EDA
  data_processing_eda:
    runs-on: ubuntu-latest
    # This job needs the `data_scraping` job to complete successfully
    needs: data_scraping
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Raw Stock Data Artifact
        uses: actions/download-artifact@v4 # Action to download artifacts from previous jobs
        with:
          name: raw-stock-data
          path: data/ # Downloads the artifact into the specified directory

      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install pipenv
        run: pip install pipenv

      - name: Install project dependencies
        run: pipenv install --deploy --system

      - name: Create plots directory
        run: mkdir -p data/plots # Ensure the plots directory exists for EDA outputs

      - name: Run Data Cleaning and EDA Script
        run: pipenv run python data_cleaning_EDA.py

      - name: Upload Processed Data and EDA Plots Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: processed-data-and-eda-plots
          path: |
            data/processed_data.csv
            data/plots/ # Upload the entire plots directory
          retention-days: 1

  # Job 3: Model Training
  model_training:
    runs-on: ubuntu-latest
    # This job needs the `data_processing_eda` job to complete successfully
    needs: data_processing_eda
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Processed Data Artifact
        uses: actions/download-artifact@v4
        with:
          name: processed-data-and-eda-plots
          path: data/ # Downloads into the data/ directory

      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install pipenv
        run: pip install pipenv

      - name: Install project dependencies
        run: pipenv install --deploy --system

      - name: Create ML Artifacts directory
        run: mkdir -p ml_artifacts # Ensure directory exists for saving models

      - name: Run Model Training Script
        run: pipenv run python model_training.py

      - name: Upload Trained Models Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: trained-models
          path: ml_artifacts/prophet_model_*.joblib # Wildcard to upload all individual Prophet models
          retention-days: 1

  # Job 4: Forecasting and Reporting
  forecasting_reporting:
    runs-on: ubuntu-latest
    # This job needs the `model_training` job to complete successfully
    needs: model_training
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Processed Data Artifact
        # Needed for ml_forecasting.py to access df_historical (processed_data.csv)
        uses: actions/download-artifact@v4
        with:
          name: processed-data-and-eda-plots
          path: data/

      - name: Download Trained Models Artifacts
        uses: actions/download-artifact@v4
        with:
          name: trained-models
          path: ml_artifacts/ # Downloads all saved models

      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install pipenv
        run: pip install pipenv

      - name: Install project dependencies
        run: pipenv install --deploy --system

      - name: Run Forecasting and Reporting Script
        run: pipenv run python ml_forecasting.py

      - name: Upload Final Report Plots
        uses: actions/upload-artifact@v4
        with:
          name: final-plots-report # Name for the final artifact group
          path: ml_artifacts/*.png # Upload all generated PNG plots
          retention-days: 7 # Keep these final outputs longer
