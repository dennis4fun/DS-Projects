# Sample CI Project: Stock Price Forecasting Project

This repository contains a data science project focused on fetching, cleaning, analyzing, and forecasting stock prices using the Prophet time-series model. The project is structured into distinct Python scripts for clarity and maintainability, making it suitable for a CI/CD pipeline.

## Project Structure:

```bash
DS-Projects/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ main.yml                  # GitHub Actions workflow for CI/CD
â”œâ”€â”€ CI-DS-Forecasting/
â”‚   â”œâ”€â”€ get_stock_data.py             # Script for data collection
â”‚   â”œâ”€â”€ data_cleaning_EDA.py          # Script for data cleaning and EDA
â”‚   â”œâ”€â”€ model_training.py             # Script for training Prophet models
â”‚   â””â”€â”€ ml_forecasting.py             # Script for generating forecasts and reports
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_stock_data.csv            # Raw data (generated by get_stock_data.py)
â”‚   â”œâ”€â”€ processed_data.csv            # Cleaned data (generated by data_cleaning_EDA.py)
â”‚   â””â”€â”€ plots/                        # Directory for EDA visualizations (generated by data_cleaning_EDA.py)
â”œâ”€â”€ ml_artifacts/
â”‚   â”œâ”€â”€ prophet_model_AAPL.joblib     # Example trained model (generated by model_training.py)
â”‚   â”œâ”€â”€ prophet_model_MSFT.joblib     # ... and other ticker models
â”‚   â”œâ”€â”€ historical_and_future_forecast.png # Forecast plot (generated by ml_forecasting.py)
â”‚   â”œâ”€â”€ future_forecast_only.png      # Forecast plot (generated by ml_forecasting.py)
â”‚   â””â”€â”€ quarterly_forecast_changes.png # Forecast plot (generated by ml_forecasting.py)
â”œâ”€â”€ Pipfile                           # Project dependencies defined here
â”œâ”€â”€ Pipfile.lock                      # Exact versions of dependencies locked here
â””â”€â”€ README.md                         # Project overview and documentation
```

- `.github/:` Contains GitHub Actions workflow configurations for CI/CD.

- `CI-DS-Forecasting/:` Dedicated subdirectory holding all core Python scripts for the data pipeline.

- `data/:` Directory (created by scripts at the root level) to store raw and processed data.

- `ml_artifacts/:` Directory (created by scripts at the root level) to store trained models (Prophet) and generated plots.

- `Pipfile & Pipfile.lock:` For reproducible dependency management using Pipenv.

- `README.md:` Project overview and instructions.

## ğŸ“¦ Python Environment Setup with Pipenv

This project uses [Pipenv](https://pipenv.pypa.io/en/latest/) for streamlined dependency management and isolated virtual environments, ensuring our development setup is always consistent.

Prerequisites
To get started, make sure you have pipenv installed on your system:

```bash
pip install --user pipenv
```

Getting Started (For Cloned Repositories)

If you've just cloned this repository, navigate to the project's root directory in your terminal and run:

```bash
pipenv install
```

This command will automatically create the necessary virtual environment and install all project dependencies as specified in the `Pipfile.lock` file.

## ğŸ“Š Data Pipeline Overview

This project includes two main Python scripts that form the initial data pipeline:

### 1. `get_stock_data.py` **(Data Collection)**

This script is responsible for fetching historical stock price data for a predefined list of S&P 500 companies.

- **Purpose:** Web scrapes daily historical stock data (Open, High, Low, Close, Volume, Adjusted Close) from a reliable online source (using `yfinance`).

- **Output:** Creates a `data/raw_stock_data.csv` file within the project's root data/ directory. This CSV contains all collected stock data in a "long" format, where each row represents a daily record for a specific stock, clearly identified by a 'Ticker' column.

- **Overwrite Policy:** Automatically deletes any existing `raw_stock_data.csv` file before a new run to ensure a fresh dataset.

- **Robustness:** Includes retry mechanisms for failed data fetches and handles potential column inconsistencies from the data source.

**To run this script:**

```bash
pipenv run python CI-DS-Forecasting/get_stock_data.py

```

### 2. `data_cleaning_EDA.py` **(Data Cleaning & EDA)**

This script takes the raw stock data, performs essential cleaning and transformations, and generates insightful visualizations.

- **Purpose:** Loads the `raw_stock_data.csv`, converts data types (e.g., 'Date' to datetime, prices to numeric), handles missing values, and performs basic data quality checks.

- **Transformations:** Common data prep cleaning.

  - **Ensures** 'Date' and all price/volume columns are in the correct data types.

  - **Fills missing values** (e.g., stock market holidays) using forward and backward fill within each stock's data.

  - **Calculates** 'Daily Return' as a new feature for analysis.

- **Visualizations:** Generates and saves the following charts into the `data/plots/` directory:

  - **Stock Price Timeline:** Visualizes the Adjusted Closing Price for all selected stocks over time, allowing for easy comparison.

  - **Daily Trading Volume:** Shows the trading volume trends for each stock.

  - **Distribution of Daily Returns:** Provides histograms to understand the return volatility for each stock.

- **Output:** Saves the cleaned and processed data into `data/processed_data.csv`. All generated plots are saved as PNG image files in the `data/plots/` subdirectory.

  - **Overwrite Policy:** Automatically overwrites processed_data.csv and the plot image files on each run.

**To run this script:**

```bash
pipenv run python CI-DS-Forecasting/get_stock_data.py
```

## ğŸš€ Running the Project Scripts:

Follow these steps sequentially to run the full forecasting pipeline:

### 1. Fetch Raw Stock Data:

This script scrapes historical stock data for predefined tickers (e.g., S&P 500 components) and saves it as `raw_stock_data.cs`v in the `data/` directory.

```bash
pipenv run python CI-DS-Forecasting/get_stock_data.py
```

### 2. Clean & Perform EDA:

This script loads the raw stock data, performs necessary cleaning and preprocessing steps, conducts Exploratory Data Analysis (EDA), and saves the processed data as `processed_data.csv` in the `data/ directory`. It also generates several EDA plots (e.g., daily returns distribution) in `data/plots/.`

```bash
pipenv run python CI-DS-Forecasting/data_cleaning_EDA.py
```

### 3. Train Forecasting Models:

This script utilizes the `processed_data.csv` to train individual Prophet time-series models for each stock ticker. The trained models are then saved as `.joblib` files in the `ml_artifacts/` directory.

```bash
pipenv run python CI-DS-Forecasting/model_training.py
```

### 4. Generate Stock Price Forecasts & Reports:

This script loads the trained Prophet models, generates future stock price forecasts until the end of 2025, and produces three types of visualizations in the `ml_artifacts/` directory:

- `historical_and_future_forecast.png:` Shows historical prices along with future forecasts and confidence intervals.

- `future_forecast_only.png:` A zoomed-in view of only the future forecasts and their confidence intervals.

- `quarterly_forecast_changes.png:` A horizontal bar graph comparing projected quarterly percentage changes for each stock.

```bash
pipenv run python CI-DS-Forecasting/ml_forecasting.py
```

### â­ CI/CD Showcase & Future Enhancements:

This project serves as a practical example of implementing a **Continuous Integration (CI)** and **Continuous Deployment (CD)** pipeline for a data science workflow using `GitHub Actions`. The `.github/workflows/main.yml` file orchestrates the execution of these four Python scripts across distinct jobs, ensuring data dependencies are met and artifacts are passed seamlessly. This setup demonstrates how to automate the end-to-end data processing, model training, and forecasting steps, providing consistency and reproducibility.

While this is a functional first example showcasing the CI process in 4 jobs using a GitHub workflow, the project has many exciting avenues for future development, including:

- **Advanced Feature Engineering:** Incorporating additional variables and external regressors (e.g., news sentiment, macroeconomic indicators, Google Trends data) into the Prophet model for potentially more accurate forecasting.

- **Interactive User Interface (UI):** Developing a web-based UI (e.g., using Streamlit, Flask, or React) to allow users to interact with the forecasts, select tickers, and view dynamic visualizations.

- **Real-Automated Report Generation:** Integrating with powerful Large Language Models (LLMs) (potentially self-hosted or via API) to automatically generate comprehensive financial reports based on the forecast results, offering textual analysis and insights beyond just charts and enabling natural language query prompts.

- **Deployment Automation:** Extending the CI pipeline to include Continuous Delivery/Deployment (CD) to automatically deploy trained models to an inference service or updated dashboards.
