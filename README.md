# Sample CI Project: Stock Price Forecasting Project

This repository contains a data science project focused on fetching, cleaning, analyzing, and forecasting stock prices using the Prophet time-series model. The project is structured into distinct Python scripts for clarity and maintainability, making it suitable for a CI/CD pipeline.

## Project Structure:

```bash
DS-Projects/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ main.yml                  # GitHub Actions workflow for CI/CD
‚îú‚îÄ‚îÄ CI-DS-Forecasting/
‚îÇ   ‚îú‚îÄ‚îÄ get_stock_data.py             # Script for data collection
‚îÇ   ‚îú‚îÄ‚îÄ data_cleaning_EDA.py          # Script for data cleaning and EDA
‚îÇ   ‚îú‚îÄ‚îÄ model_training.py             # Script for training Prophet models
‚îÇ   ‚îú‚îÄ‚îÄ ml_forecasting.py             # Script for generating forecasts and reports
|   ‚îî‚îÄ‚îÄ streamlit_app.py              # Streamlit UI application
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw_stock_data.csv            # Raw data (generated by get_stock_data.py)
‚îÇ   ‚îú‚îÄ‚îÄ processed_data.csv            # Cleaned data (generated by data_cleaning_EDA.py)
‚îÇ   ‚îî‚îÄ‚îÄ plots/                        # Directory for EDA visualizations (generated by data_cleaning_EDA.py)
‚îú‚îÄ‚îÄ ml_artifacts/
‚îÇ   ‚îú‚îÄ‚îÄ prophet_model_AAPL.joblib     # Example trained model (generated by model_training.py)
‚îÇ   ‚îú‚îÄ‚îÄ prophet_model_MSFT.joblib     # ... and other ticker models
‚îÇ   ‚îú‚îÄ‚îÄ historical_and_future_forecast.png # Forecast plot (generated by ml_forecasting.py)
‚îÇ   ‚îú‚îÄ‚îÄ future_forecast_only.png      # Forecast plot (generated by ml_forecasting.py)
‚îÇ   ‚îî‚îÄ‚îÄ quarterly_forecast_changes.png # Forecast plot (generated by ml_forecasting.py)
‚îú‚îÄ‚îÄ .env                              # Local environment variables (NOT committed to Git)
‚îú‚îÄ‚îÄ Pipfile                           # Project dependencies defined here
‚îú‚îÄ‚îÄ Pipfile.lock                      # Exact versions of dependencies locked here
‚îî‚îÄ‚îÄ README.md                         # Project overview and documentation
```

- `.github/:` Contains GitHub Actions workflow configurations for CI/CD.

- `CI-DS-Forecasting/:` Dedicated subdirectory holding all core Python scripts for the data pipeline.

- `data/:` Directory (created by scripts at the root level) to store raw and processed data.

- `ml_artifacts/:` Directory (created by scripts at the root level) to store trained models (Prophet) and generated plots.
- `.env:` Used for managing environment variables (like API keys) securely. .env is ignored by Git, in my repo so you have to create your own and put your api & token keys there.

- `Pipfile & Pipfile.lock:` For reproducible dependency management using Pipenv.

- `README.md:` Project overview and instructions.

## üì¶ Python Environment Setup with Pipenv

This project uses [Pipenv](https://pipenv.pypa.io/en/latest/) for streamlined dependency management and isolated virtual environments, ensuring our development setup is always consistent.

Prerequisites
To get started, make sure you have pipenv installed on your system:

```bash
pip install --user pipenv
```

Getting Started (For Cloned Repositories)

If you've just cloned this repository, navigate to the project's root directory in your terminal and run:

```bash
pipenv install
```

This command will automatically create the necessary virtual environment and install all project dependencies as specified in the `Pipfile.lock` file.

## üìä Data Pipeline Overview

This project includes two main Python scripts that form the initial data pipeline:

### 1. `get_stock_data.py` **(Data Collection)**

This script is responsible for fetching historical stock price data for a predefined list of S&P 500 companies.

- **Purpose:** Web scrapes daily historical stock data (Open, High, Low, Close, Volume, Adjusted Close) from a reliable online source (using `yfinance`).

- **Output:** Creates a `data/raw_stock_data.csv` file within the project's root data/ directory. This CSV contains all collected stock data in a "long" format, where each row represents a daily record for a specific stock, clearly identified by a 'Ticker' column.

- **Overwrite Policy:** Automatically deletes any existing `raw_stock_data.csv` file before a new run to ensure a fresh dataset.

- **Robustness:** Includes retry mechanisms for failed data fetches and handles potential column inconsistencies from the data source.

**To run this script:**

```bash
pipenv run python CI-DS-Forecasting/get_stock_data.py

```

### 2. `data_cleaning_EDA.py` **(Data Cleaning & EDA)**

This script takes the raw stock data, performs essential cleaning and transformations, and generates insightful visualizations.

- **Purpose:** Loads the `raw_stock_data.csv`, converts data types (e.g., 'Date' to datetime, prices to numeric), handles missing values, and performs basic data quality checks.

- **Transformations:** Common data prep cleaning.

  - **Ensures** 'Date' and all price/volume columns are in the correct data types.

  - **Fills missing values** (e.g., stock market holidays) using forward and backward fill within each stock's data.

  - **Calculates** 'Daily Return' as a new feature for analysis.

- **Visualizations:** Generates and saves the following charts into the `data/plots/` directory:

  - **Stock Price Timeline:** Visualizes the Adjusted Closing Price for all selected stocks over time, allowing for easy comparison.

  - **Daily Trading Volume:** Shows the trading volume trends for each stock.

  - **Distribution of Daily Returns:** Provides histograms to understand the return volatility for each stock.

- **Output:** Saves the cleaned and processed data into `data/processed_data.csv`. All generated plots are saved as PNG image files in the `data/plots/` subdirectory.

  - **Overwrite Policy:** Automatically overwrites processed_data.csv and the plot image files on each run.

**To run this script:**

```bash
pipenv run python CI-DS-Forecasting/get_stock_data.py
```

## üöÄ Running the Project Scripts:

Follow these steps sequentially to run the full forecasting pipeline:

### 1. Fetch Raw Stock Data:

This script scrapes historical stock data for predefined tickers (e.g., S&P 500 components) and saves it as `raw_stock_data.cs`v in the `data/` directory.

```bash
pipenv run python CI-DS-Forecasting/get_stock_data.py
```

### 2. Clean & Perform EDA:

This script loads the raw stock data, performs necessary cleaning and preprocessing steps, conducts Exploratory Data Analysis (EDA), and saves the processed data as `processed_data.csv` in the `data/ directory`. It also generates several EDA plots (e.g., daily returns distribution) in `data/plots/.`

```bash
pipenv run python CI-DS-Forecasting/data_cleaning_EDA.py
```

### 3. Train Forecasting Models:

This script utilizes the `processed_data.csv` to train individual Prophet time-series models for each stock ticker. The trained models are then saved as `.joblib` files in the `ml_artifacts/` directory.

```bash
pipenv run python CI-DS-Forecasting/model_training.py
```

### 4. Generate Stock Price Forecasts & Reports:

This script loads the trained Prophet models, generates future stock price forecasts until the end of 2025, and produces three types of visualizations in the `ml_artifacts/` directory:

- `historical_and_future_forecast.png:` Shows historical prices along with future forecasts and confidence intervals.

- `future_forecast_only.png:` A zoomed-in view of only the future forecasts and their confidence intervals.

- `quarterly_forecast_changes.png:` A horizontal bar graph comparing projected quarterly percentage changes for each stock.

```bash
pipenv run python CI-DS-Forecasting/ml_forecasting.py
```

## 5 üìà Interactive UI Dashboard (Streamlit)

This project now includes an interactive Streamlit dashboard (`CI-DS-Forecasting/streamlit_app.py`) for a more user-friendly experience. It allows you to visualize data and forecasts, run the pipeline locally, and check GitHub Actions status directly through a web interface.

**Local Setup for UI:**

1. **Environment Variables** (`.env`):

   - This app requires a GitHub Personal Access Token (PAT) to fetch workflow status. For local development, this token should be stored in a `.env` file to keep it secure and out of version control.

   - Create your `.env` file:

   - Edit .env: Open the newly created .env file and replace the placeholder with your actual GitHub Personal Access Token:

   - `GITHUB_TOKEN=your_copied_github_token_here`

   - (Remember to add .env to your .gitignore to prevent accidental commitment of your token.)

2. **Obtaining a GitHub Personal Access Token (PAT):**

   - Go to your GitHub profile settings: Settings > Developer settings > Personal access tokens > `Tokens (classic)`.

   - Click "Generate new token `(classic)`".

   - Give it a descriptive name (e.g., "Streamlit Dashboard Access").

   - Crucially, grant it the repo scope (this allows it to read repository metadata and workflow runs). You can set an expiration for added security.

   - Generate the token and copy it immediately as it will not be shown again.

3. **Run the Streamlit App:**

   - Ensure all project dependencies (including `streamlit`, `requests`, and `python-dotenv`) are installed in your `Pipenv` environment by running `pipenv install` from the project root (DS-Projects/).

   - Navigate your terminal to the project root directory (DS-Projects/).

   - Execute the `Streamlit` application using `Pipenv`, specifying the full path to the `streamlit_app.py` script and a specific port to avoid conflicts:

   - pipenv run streamlit run `CI-DS-Forecasting/streamlit_app.py --server.port 8502`

   - The app will open in your default web browser (usually at `http://localhost:8502`).

### ‚≠ê CI/CD Showcase & Future Enhancements:

This project serves as a practical example of implementing a **Continuous Integration (CI)** and **Continuous Deployment (CD)** pipeline for a data science workflow using `GitHub Actions`. The `.github/workflows/main.yml` file orchestrates the execution of these four Python scripts across distinct jobs, ensuring data dependencies are met and artifacts are passed seamlessly. This setup demonstrates how to automate the end-to-end data processing, model training, and forecasting steps, providing consistency and reproducibility.

While this is a functional first example showcasing the CI process in 4 jobs using a GitHub workflow, the project has many exciting avenues for future development, including:

- **Advanced Feature Engineering:** Incorporating additional variables and external regressors (e.g., news sentiment, macroeconomic indicators, Google Trends data) into the Prophet model for potentially more accurate forecasting.

- **Interactive User Interface (UI):** Developing a web-based UI (e.g., using Streamlit, Flask, or React) to allow users to interact with the forecasts, select tickers, and view dynamic visualizations.

- **Real-Automated Report Generation:** Integrating with powerful Large Language Models (LLMs) (potentially self-hosted or via API) to automatically generate comprehensive financial reports based on the forecast results, offering textual analysis and insights beyond just charts and enabling natural language query prompts.

- **Deployment Automation:** Extending the CI pipeline to include Continuous Delivery/Deployment (CD) to automatically deploy trained models to an inference service or updated dashboards.

### üìú Version History

**1.0.0 - Initial Release & Core Enhancements (June 21, 2025)**
This marks the initial major release, focusing on establishing a robust data pipeline and improving forecasting capabilities.

Key Updates:

- Expanded Data Collection (`get_stock_data.py`):

  - Added fetching of macroeconomic indicators (VIX Index and 10-Year US Treasury Yield - TNX) alongside stock data.

  - Enhanced robustness for data fetching, including improved `Ticker` column handling and `Adj Close` population.

- Enhanced Data Cleaning & EDA (`data_cleaning_EDA.py`):

  - Implemented more robust NaN handling with `ffill()/bfill()` grouped by ticker for stock data and globally for macroeconomic data.

  - Introduced filtering to remove tickers with insufficient data points.

  - Added new EDA visualizations for VIX and TNX data.

  - Ensured generation of a non-empty `processed_data.csv`.

- Improved Model Training (`model_training.py`):

  - Integrated 'Open', 'High', 'Low', 'Close', 'Volume', 'VIX_Close', and 'TNX_Close' as external regressors into the Prophet models.

  - Adjusted Prophet hyperparameters (changepoint_prior_scale) for increased flexibility.

- Refined ML Forecasting & Reporting (ml_forecasting.py):

  - Implemented a rolling average strategy for future regressor values to provide slightly more dynamic forecasts.

  - Corrected plot saving location: All forecast plots (`historical_and_future_forecast.png`, `future_forecast_only.png`, `quarterly_forecast_changes.png`) are now correctly saved to the `ml_artifacts`/ directory.

  - Enhanced Plot Clarity: Increased visibility of confidence intervals, zoomed Y-axis for future forecasts, and added clear green/red color-coding to the quarterly changes plot to indicate projected gains/losses.

- Automated Weekly Pipeline (`.github/workflows/main.yml`):

  - Configured the GitHub Actions workflow to run automatically every Friday at 5:00 PM EST (22:00 UTC), ensuring weekly updates based on the latest market close data.

  - Adjusted artifact upload paths to ensure all relevant data and plots (including EDA plots) are available from GitHub Actions runs.

**1.1.0 - Interactive UI Integration (June 22, 2025)**
This release introduces an interactive Streamlit-based User Interface to enhance project usability and visibility of results.

**Key Updates:**

- **New Streamlit Application** (`CI-DS-Forecasting/streamlit_app.py`):

  - Provides a web-based dashboard for viewing forecast plots, EDA visualizations, and raw/processed data files.

  - Includes an interactive feature to trigger the entire data pipeline locally, updating all data and plots.

  - Integrates with GitHub Actions API to display the status of the latest CI/CD workflow run.

- **Secure Environment Variable Handling:**

  - Introduced `python-dotenv` and instructions for creating a `.env` file for secure management of API keys (like GitHub PAT) for local development, preventing sensitive information from being committed to the repository.

  - Updated `README.md` with clear instructions on .env setup and GitHub Personal Access Token generation.

- **Data File Viewer in UI:** Added a dedicated section in the `Streamlit app` to view `raw_stock_data.csv` and `processed_data.csv` with filtering capabilities by ticker and date range.
